{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import Conflict\n",
        "from google.cloud.exceptions import NotFound\n",
        "import random\n",
        "\n",
        "DEFAULT_NULLABLE_THRESHOLD = 0.9\n",
        "DEFAULT_UNIQUE_THRESHOLD = 0.9"
      ],
      "metadata": {
        "id": "BUkaKgX54O29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_dataset_exists(client, project_id, dataset_name):\n",
        "    dataset_ref = client.dataset(dataset_name)\n",
        "    try:\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        client.create_dataset(dataset)\n",
        "        print(f'Dataset {dataset_name} created.')\n",
        "    except Conflict:\n",
        "        print(f\"Dataset '{dataset_name}' already exists.\")\n",
        "\n",
        "\n",
        "def get_partitioning_info(client, project_id, dataset_name, table_name):\n",
        "    table_ref = client.dataset(dataset_name).table(table_name)\n",
        "    table = client.get_table(table_ref)  # API call\n",
        "\n",
        "    partition_info = {\n",
        "        \"is_partitioned\": False,\n",
        "        \"field\": None,\n",
        "        \"type\": None\n",
        "    }\n",
        "\n",
        "    if table.time_partitioning:\n",
        "        partition_info[\"is_partitioned\"] = True\n",
        "        partition_info[\"field\"] = table.time_partitioning.field\n",
        "        partition_info[\"type\"] = str(table.time_partitioning.type_).split('.')[-1]  # Will give \"DAY\" for day partitioned\n",
        "    elif table.range_partitioning:\n",
        "        partition_info[\"is_partitioned\"] = True\n",
        "        partition_info[\"field\"] = table.range_partitioning.field\n",
        "        partition_info[\"type\"] = \"RANGE\"\n",
        "\n",
        "    return partition_info\n",
        "\n",
        "\n",
        "def get_all_tables_in_dataset(client, project_id, dataset_name):\n",
        "    tables = list(client.list_tables(dataset_name))\n",
        "    return [table.table_id for table in tables]\n",
        "\n",
        "\n",
        "def get_table_row_count(client, project_id, dataset_name, table_name):\n",
        "    table_ref = f\"{project_id}.{dataset_name}.{table_name}\"\n",
        "    table = client.get_table(table_ref)\n",
        "    return table.num_rows\n",
        "\n",
        "\n",
        "def table_exists(client, project_id, dataset_name, table_name):\n",
        "    table_ref = f\"{project_id}.{dataset_name}.{table_name}\"\n",
        "    try:\n",
        "        client.get_table(table_ref)\n",
        "        return True\n",
        "    except NotFound:\n",
        "        return False"
      ],
      "metadata": {
        "id": "Ah7D5QYd4UAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_subset_table(client, project_id, dataset_name, table_name, sample_percentage):\n",
        "    subset_table_name = f\"{table_name}_subset\"\n",
        "\n",
        "    # Check if the subset table already exists\n",
        "    existing_tables = [table.table_id for table in client.list_tables(f\"{project_id}.sampling\")]\n",
        "    if subset_table_name in existing_tables:\n",
        "        print(f\"Sample table {subset_table_name} already exists. Skipping table creation.\")\n",
        "        return subset_table_name\n",
        "\n",
        "    # Ensure the 'sampling' dataset exists\n",
        "    ensure_dataset_exists(client, project_id, \"sampling\")\n",
        "\n",
        "    partition_info = get_partitioning_info(client, project_id, dataset_name, table_name)\n",
        "\n",
        "    # If table is partitioned by DAY\n",
        "    if partition_info[\"is_partitioned\"] and partition_info[\"type\"] == \"DAY\":\n",
        "        subset_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE `{project_id}.sampling.{subset_table_name}` AS\n",
        "        SELECT *\n",
        "        FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE {partition_info[\"field\"]} = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)\n",
        "        TABLESAMPLE SYSTEM ({sample_percentage:.2f} PERCENT)\n",
        "        \"\"\"\n",
        "\n",
        "    # If table is partitioned by RANGE (Integer assumed)\n",
        "    elif partition_info[\"is_partitioned\"] and partition_info[\"type\"] == \"RANGE\":\n",
        "        # Fetch a single partition ID using the INFORMATION_SCHEMA.PARTITIONS\n",
        "        partition_values_query = f\"\"\"\n",
        "        SELECT partition_id\n",
        "        FROM `{project_id}.{dataset_name}.INFORMATION_SCHEMA.PARTITIONS`\n",
        "        WHERE table_name = '{table_name}'\n",
        "        LIMIT 1\n",
        "        \"\"\"\n",
        "        query_job = client.query(partition_values_query)\n",
        "        results = list(query_job.result())\n",
        "\n",
        "        # If there are no partition values, exit\n",
        "        if not results:\n",
        "            print(\"No partition values found. Cannot proceed.\")\n",
        "            return\n",
        "\n",
        "        # Get the partition value from the result\n",
        "        chosen_partition = results[0].partition_id\n",
        "\n",
        "        subset_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE `{project_id}.sampling.{subset_table_name}` AS\n",
        "        SELECT *\n",
        "        FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE {partition_info[\"field\"]} = {chosen_partition}\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        # If not partitioned or another type of partitioning\n",
        "        subset_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE `{project_id}.sampling.{subset_table_name}` AS\n",
        "        SELECT *\n",
        "        FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "        TABLESAMPLE SYSTEM ({sample_percentage:.2f} PERCENT)\n",
        "        \"\"\"\n",
        "\n",
        "    print(f\"Creating sample table: {subset_table_name}\")\n",
        "    query_job = client.query(subset_query)\n",
        "    query_job.result()\n",
        "    print(f\"Sample table {subset_table_name} created successfully.\")\n",
        "\n",
        "    return subset_table_name\n"
      ],
      "metadata": {
        "id": "QF75XqKfOUkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_run_profiling(client, project_id, dataset_name, table_name, nullable_threshold, unique_threshold, profiling_dataset, batch_size=10):\n",
        "\n",
        "    # Step 1: Get all the columns to be profiled\n",
        "    columns_query = f\"\"\"\n",
        "    SELECT column_name, data_type\n",
        "    FROM `{project_id}.{dataset_name}.INFORMATION_SCHEMA.COLUMNS`\n",
        "    WHERE table_name = '{table_name}'\n",
        "    AND data_type IN ('STRING', 'BOOL', 'TIMESTAMP', 'DATE', 'INT64', 'NUMERIC', 'FLOAT64')\n",
        "    \"\"\"\n",
        "    query_job = client.query(columns_query)\n",
        "    columns = list(query_job.result())\n",
        "\n",
        "    # Step 2: Divide the columns into chunks\n",
        "    column_chunks = [columns[i:i+batch_size] for i in range(0, len(columns), batch_size)]\n",
        "\n",
        "    temp_tables = []\n",
        "\n",
        "    for index, column_chunk in enumerate(column_chunks):\n",
        "        # Build the query for the current chunk\n",
        "        query = build_profiling_query_for_chunk(client, project_id, dataset_name, table_name, nullable_threshold, unique_threshold, column_chunk)\n",
        "\n",
        "        # Define a temp table name for the current chunk\n",
        "        temp_table_name = f\"{project_id}.{profiling_dataset}.temp_{table_name}_{index}\"\n",
        "        temp_tables.append(temp_table_name)\n",
        "\n",
        "        # Run the query and store the result in the temp table\n",
        "        job_config = bigquery.QueryJobConfig(destination=temp_table_name)\n",
        "        client.query(query, job_config=job_config).result()\n",
        "\n",
        "    # Step 3: Union all the temp tables into the final table\n",
        "    union_query = \" UNION ALL \".join([f\"SELECT * FROM {temp_table}\" for temp_table in temp_tables])\n",
        "    final_table_name = f\"{project_id}.{profiling_dataset}.profiling_results_{table_name}\"\n",
        "    job_config = bigquery.QueryJobConfig(destination=final_table_name)\n",
        "    client.query(union_query, job_config=job_config).result()\n",
        "\n",
        "    # Step 4: Clean up - Delete all the temp tables\n",
        "    for temp_table in temp_tables:\n",
        "        client.delete_table(temp_table, not_found_ok=True)\n"
      ],
      "metadata": {
        "id": "jQlmvD64Ic1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_profiling_query_for_chunk(client, project_id, dataset_name, table_name, nullable_threshold, unique_threshold, column_chunk):\n",
        "\n",
        "    partition_info = get_partitioning_info(client, project_id, dataset_name, table_name)\n",
        "\n",
        "    subqueries = []\n",
        "    for column in column_chunk:\n",
        "      subquery = f\"\"\"\n",
        "      SELECT\n",
        "          '{column.column_name}' AS column_name,\n",
        "          '{column.data_type}' AS data_type,\n",
        "          CAST(MIN({column.column_name}) AS STRING) AS min_val,\n",
        "          CAST(MAX({column.column_name}) AS STRING) AS max_val,\n",
        "          COUNTIF({column.column_name} IS NULL) AS null_count,\n",
        "          COUNTIF({column.column_name} IS NOT NULL) AS not_null_count,\n",
        "          APPROX_COUNT_DISTINCT({column.column_name}) AS unique_count,\n",
        "          CAST((SELECT {column.column_name} FROM `{project_id}.{dataset_name}.{table_name}` GROUP BY {column.column_name} ORDER BY COUNT(*) DESC LIMIT 1) AS STRING) AS most_common_value,\n",
        "          (SELECT COUNT(*) FROM `{project_id}.{dataset_name}.{table_name}` GROUP BY {column.column_name} ORDER BY COUNT(*) DESC LIMIT 1) AS most_common_value_count,\n",
        "          '{partition_info[\"field\"] if partition_info[\"field\"] else 'No'}' AS partitioned_by_column,\n",
        "          '{partition_info[\"type\"] if partition_info[\"type\"] else 'No'}' AS partition_type\n",
        "      FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "      \"\"\"\n",
        "      subqueries.append(subquery)\n",
        "\n",
        "    final_query = f\"\"\"\n",
        "    WITH total AS (\n",
        "      SELECT COUNT(*) AS total_count\n",
        "      FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "    ),\n",
        "    stats AS (\n",
        "      { ' UNION ALL '.join(subqueries) }\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "      data_type,\n",
        "      column_name,\n",
        "      min_val,\n",
        "      max_val,\n",
        "      null_count,\n",
        "      not_null_count,\n",
        "      CASE WHEN (null_count/total_count) >= {nullable_threshold} THEN 'Yes' ELSE 'No' END AS suggest_not_nullable,\n",
        "      unique_count,\n",
        "      CASE WHEN (unique_count/total_count) >= {unique_threshold} THEN 'Yes' ELSE 'No' END AS suggest_unique,\n",
        "      most_common_value,\n",
        "      most_common_value_count,\n",
        "      partitioned_by_column,\n",
        "      partition_type\n",
        "    FROM stats\n",
        "    CROSS JOIN total\n",
        "    \"\"\"\n",
        "\n",
        "    return final_query\n"
      ],
      "metadata": {
        "id": "ZvMjtD35IgBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# production ready:\n",
        "\n",
        "def main():\n",
        "    project_id = 'project-id'\n",
        "    dataset_name = 'dataset-id'\n",
        "    sample_percentage = 1\n",
        "    profiling_dataset = 'data_profiling'\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Ensure the profiling dataset exists\n",
        "    ensure_dataset_exists(client, project_id, profiling_dataset)\n",
        "\n",
        "    # Get all tables in the dataset\n",
        "    all_tables = get_all_tables_in_dataset(client, project_id, dataset_name)\n",
        "\n",
        "    for table_name in all_tables:\n",
        "        # Skip table if row count is 0\n",
        "        if get_table_row_count(client, project_id, dataset_name, table_name) == 0:\n",
        "            print(f\"Skipping {table_name} as it has no records.\")\n",
        "            continue\n",
        "\n",
        "        # Check existence of sample and profile tables\n",
        "        subset_table_name_check = f\"{table_name}_subset\"\n",
        "        result_table_name_check = f\"profiling_results_{table_name}\"\n",
        "\n",
        "        if table_exists(client, project_id, \"sampling\", subset_table_name_check) and table_exists(client, project_id, profiling_dataset, result_table_name_check):\n",
        "            print(f\"Both sample and profile tables for {table_name} already exist. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing table: {table_name}\")\n",
        "\n",
        "        subset_table_name = create_subset_table(client, project_id, dataset_name, table_name, sample_percentage)\n",
        "        profiling_query = build_profiling_query(client, project_id, \"sampling\", subset_table_name, DEFAULT_NULLABLE_THRESHOLD, DEFAULT_UNIQUE_THRESHOLD, profiling_dataset)\n",
        "\n",
        "        if profiling_query is None:\n",
        "          continue\n",
        "\n",
        "        print(f\"Profiling query for table {subset_table_name} built.\")\n",
        "        print(profiling_query)\n",
        "\n",
        "        query_job = client.query(profiling_query)\n",
        "        results = query_job.result()\n",
        "\n",
        "        # Store results in a new table with a unique name per source table\n",
        "        result_table_name = f\"profiling_results_{table_name}\"\n",
        "        table_ref = client.dataset(profiling_dataset).table(result_table_name)\n",
        "        job_config = bigquery.QueryJobConfig(destination=table_ref)\n",
        "        client.query(profiling_query, job_config=job_config).result()\n",
        "        print(f\"Profiling results for table {subset_table_name} stored in {result_table_name}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fbSioQy7MnWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single table testing:\n",
        "def main():\n",
        "    project_id = 'project-id'\n",
        "    dataset_name = 'dataset-id'\n",
        "    sample_percentage = 1\n",
        "    profiling_dataset = 'data_profiling'\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Ensure the profiling dataset exists\n",
        "    ensure_dataset_exists(client, project_id, profiling_dataset)\n",
        "\n",
        "    # Get all tables in the dataset\n",
        "    all_tables = get_all_tables_in_dataset(client, project_id, dataset_name)\n",
        "\n",
        "    # Get only the first table for processing\n",
        "    table_name = all_tables[0]\n",
        "\n",
        "    # Skip table if row count is 0\n",
        "    if get_table_row_count(client, project_id, dataset_name, table_name) == 0:\n",
        "        print(f\"Skipping {table_name} as it has no records.\")\n",
        "        return\n",
        "\n",
        "    # Check existence of sample and profile tables\n",
        "    subset_table_name_check = f\"{table_name}_subset\"\n",
        "    result_table_name_check = f\"profiling_results_{table_name}\"\n",
        "\n",
        "    if table_exists(client, project_id, \"sampling\", subset_table_name_check) and table_exists(client, project_id, profiling_dataset, result_table_name_check):\n",
        "        print(f\"Both sample and profile tables for {table_name} already exist. Skipping.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing table: {table_name}\")\n",
        "\n",
        "    subset_table_name = create_subset_table(client, project_id, dataset_name, table_name, sample_percentage)\n",
        "    profiling_query = build_and_run_profiling(client, project_id, \"sampling\", subset_table_name, DEFAULT_NULLABLE_THRESHOLD, DEFAULT_UNIQUE_THRESHOLD, profiling_dataset, batch_size=10)\n",
        "\n",
        "    if profiling_query is None:\n",
        "      return\n",
        "\n",
        "    print(f\"Profiling query for table {subset_table_name} built.\")\n",
        "    print(profiling_query)\n",
        "\n",
        "    query_job = client.query(profiling_query)\n",
        "    results = query_job.result()\n",
        "\n",
        "    # Store results in a new table with a unique name per source table\n",
        "    result_table_name = f\"profiling_results_{table_name}\"\n",
        "    table_ref = client.dataset(profiling_dataset).table(result_table_name)\n",
        "    job_config = bigquery.QueryJobConfig(destination=table_ref)\n",
        "    client.query(profiling_query, job_config=job_config).result()\n",
        "    print(f\"Profiling results for table {subset_table_name} stored in {result_table_name}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "cETr1eo3SDPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}